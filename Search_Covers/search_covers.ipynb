{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "iSFGc7NIWLN9",
        "B55prNDfjLp1",
        "6tvlfMI-jLp4",
        "eFk6bS4B10bI",
        "8_PmpzoE3BB1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing"
      ],
      "metadata": {
        "id": "iSFGc7NIWLN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytubefix\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers torch faiss-cpu\n",
        "!pip install openai-whisper\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "fg-_cIO1Xefc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2913b8c-4648-4de1-890a-2c249701756e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytubefix\n",
            "  Downloading pytubefix-8.7.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Downloading pytubefix-8.7.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytubefix\n",
            "Successfully installed pytubefix-8.7.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=536adf95ea37a1af4f8f7e54267570fe01be28a4b22bc03396b6d1680b83741c\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.7.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.0 (from gradio)\n",
            "  Downloading gradio_client-1.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.0->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.5.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.7.1-py3-none-any.whl (57.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.0-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.8.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.7.1 gradio-client-1.5.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.8.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0 uvicorn-0.32.1 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Development\n"
      ],
      "metadata": {
        "id": "bCctWy6mjLpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import all the necessary libraries"
      ],
      "metadata": {
        "id": "B55prNDfjLp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytubefix import YouTube\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "import whisper\n",
        "import torch\n",
        "import torchvision\n",
        "import torchaudio\n",
        "import re\n",
        "import gradio as gr\n",
        "import os"
      ],
      "metadata": {
        "id": "44XiF6ZmjLp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed3a1cd-77df-4335-9b82-254ab2684f27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Extract lyrics as a string from a youtube url using `whisper` form `openai-whisper library` and `pytubefix` to download the audio from youtube url video\n",
        "data_URL_handling.py\n",
        "\n",
        "**Note: Issues with another enviroments**\n",
        "\n",
        "- `pytubefix` has restrictions on its use, specifically in the po_token_verifier function, which tries to read data from the standard input (input) to obtain a token necessary to access YouTube. This is incompatible with deployed applications, since there is no direct interaction with the terminal.\n",
        "\n",
        "- `yt_dlp` has the same restrictions\n",
        "\n",
        "- `Youtube Data API v3` is usefull for data extraction but it is not possible to download the video or audio for transcription\n",
        "\n",
        "This issues dont happen in Colab because automatically takes the configured Google account data\n"
      ],
      "metadata": {
        "id": "6tvlfMI-jLp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import whisper\n",
        "import torch\n",
        "from pytubefix import YouTube\n",
        "\n",
        "def is_valid_youtube_url(url):\n",
        "  \"\"\"\n",
        "  Check if a URL is valid for YouTube.\n",
        "  Args:\n",
        "      url (str): URL to be validated.\n",
        "  Returns:\n",
        "      bool: True if valid, False otherwise.\n",
        "  \"\"\"\n",
        "  youtube_regex = re.compile(\n",
        "      r\"^(https?://)?(www\\.)?(youtube\\.com|youtu\\.?be)/.+$\"\n",
        "  )\n",
        "  return youtube_regex.match(url) is not None\n",
        "\n",
        "def get_details_from_youtube_url(youtube_url):\n",
        "  \"\"\"\n",
        "    Extract the lyrics from a YouTube video using openai-whisper.\n",
        "\n",
        "    Args:\n",
        "        youtube_url: The URL of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary with the video details and the transcribed lyrics.\n",
        "    \"\"\"\n",
        "  if not is_valid_youtube_url(youtube_url):\n",
        "    raise ValueError(f\"Error: The URL '{youtube_url}' is invalid. Tray another\")\n",
        "\n",
        "  try:\n",
        "    # change to cpu in local or hf space\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = whisper.load_model(\"base\", device=device)  #large-v2, large\n",
        "\n",
        "    # yt = YouTube(youtube_url,use_po_token=True,use_oauth=True, allow_oauth_cache=True) # for another enviroments like hf space\n",
        "    yt = YouTube(youtube_url)\n",
        "\n",
        "    # Prepare video details\n",
        "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
        "\n",
        "    if not audio_stream:\n",
        "        raise ValueError(\"No audio stream found for the video.\")\n",
        "\n",
        "    video_details = {\n",
        "      \"title\": yt.title,\n",
        "      \"author\": yt.author,\n",
        "      \"audio_url\": audio_stream.url,\n",
        "      \"lyrics\": \"\"\n",
        "      }\n",
        "\n",
        "    # Transcribe lyrics\n",
        "    lyrics = model.transcribe(video_details['audio_url'])\n",
        "    lyrics = [segment[\"text\"] for segment in lyrics[\"segments\"]] # divided in segments of sentences\n",
        "    # lyrics = lyrics[\"text\"]  # complete song in a text\n",
        "    video_details['lyrics'] = lyrics\n",
        "    video_details.pop('audio_url') # Remove temporary audio URL\n",
        "\n",
        "    return video_details\n",
        "\n",
        "  except Exception as e:\n",
        "    raise ValueError(f\"Error processing YouTube URL {youtube_url}: {e}\")\n"
      ],
      "metadata": {
        "id": "SEfIqgwWjLp5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Song Database"
      ],
      "metadata": {
        "id": "8_PmpzoE3BB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# from data_URL_handling import get_details_from_youtube_url\n",
        "\n",
        "class SongDatabase:\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.song_data = []  # List to store song metadata (title, author, lyrics)\n",
        "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    def add_song(self, url_yt):\n",
        "        \"\"\"\n",
        "        Adds a song to the database by extracting its lyrics and embeddings.\n",
        "        Args:\n",
        "            youtube_url (str): The YouTube URL of the song.\n",
        "        Returns:\n",
        "            bool: True if the song was added successfully, False otherwise. -- There was adapted to raise exceptions for gradio app\n",
        "        \"\"\"\n",
        "        # Extract lyrics from song url\n",
        "        song = get_details_from_youtube_url(url_yt)\n",
        "\n",
        "        if not song or not song[\"lyrics\"]:\n",
        "          raise ValueError(\"There was an error trying to extract the data from the URL\")\n",
        "\n",
        "        if self.song_exists(song['title'],song['author']):\n",
        "          raise ValueError(\"The song already exists\")\n",
        "\n",
        "        lyrics = song['lyrics']\n",
        "\n",
        "        # Extract the embedidngs of the lyrics\n",
        "        embeddings = self.extract_embeddings([lyrics])\n",
        "\n",
        "        if embeddings is None:\n",
        "          raise ValueError(\"There was an error trying to extract the embeddings from the data lyrics\")\n",
        "\n",
        "        # Initialize FAISS index if not already created\n",
        "        if self.index is None:\n",
        "            self.index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "        # IndexFlatIP, which is an index based on internal product vectors and does not support direct elimination\n",
        "\n",
        "        # Add to index and store song details\n",
        "        self.index.add(embeddings)\n",
        "        self.song_data.append({\n",
        "            'title': song['title'],\n",
        "            'author': song['author'],\n",
        "            'lyrics': lyrics\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return True\n",
        "\n",
        "    def song_exists(self, title,author):\n",
        "      \"\"\"\n",
        "      Checks if a song exists in the database by its title.\n",
        "      Args:\n",
        "          title (str): The title of the song to check.\n",
        "          author (str): The author of the song to check.\n",
        "      Returns:\n",
        "          bool: True if the song exists, False otherwise.\n",
        "      \"\"\"\n",
        "      return any(song[\"title\"] == title and song['author']==author for song in self.song_data)\n",
        "\n",
        "\n",
        "    def extract_embeddings(self, texts):\n",
        "      \"\"\"\n",
        "        Extracts normalized embeddings for the given texts.\n",
        "        Args:\n",
        "            texts (list of str): List of texts to encode.\n",
        "        Returns:\n",
        "            np.ndarray: Normalized embeddings.\n",
        "        \"\"\"\n",
        "      try:\n",
        "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
        "        return normalize(embeddings, norm=\"l2\")\n",
        "      except Exception as e:\n",
        "        return f\"Error extracting embeddings: {e}\"\n",
        "\n",
        "\n",
        "    def search_covers(self, query_url, top_k):\n",
        "      \"\"\"\n",
        "        Searches for the most similar songs in the database to the song in the query URL.\n",
        "        Args:\n",
        "            query_url (str): The YouTube URL of the query song.\n",
        "            top_k (int): Number of top similar results to retrieve.\n",
        "        Returns:\n",
        "            list of dict: List of dictionaries with song title, author, and similarity score.\n",
        "        \"\"\"\n",
        "\n",
        "      if self.index is None or not self.song_data:\n",
        "        raise ValueError(\"The database is empty. Please add songs before searching.\")\n",
        "\n",
        "      query_details = get_details_from_youtube_url(query_url)\n",
        "      if not query_details or not query_details[\"lyrics\"]:\n",
        "        raise ValueError(\"There was an error trying to extract the data from the URL\")\n",
        "\n",
        "      if top_k > len(self.song_data):\n",
        "        raise ValueError(f\"Invalid 'top_k' value: {top_k}. Must be <= {len(self.song_data)}.\")\n",
        "\n",
        "      query_embedding = self.extract_embeddings([query_details[\"lyrics\"]])\n",
        "      if query_embedding is None:\n",
        "        raise ValueError(\"There was an error trying to extract the embeddings from the data lyrics\")\n",
        "\n",
        "      D, I = self.index.search(query_embedding, top_k)\n",
        "      results = []\n",
        "\n",
        "      for i, idx in enumerate(I[0]):\n",
        "        if idx < len(self.song_data):\n",
        "          song = self.song_data[idx]\n",
        "          print(song)\n",
        "          similarity = D[0][i]*100\n",
        "          results.append({\n",
        "              \"title\": song[\"title\"],\n",
        "              \"author\": song[\"author\"],\n",
        "              \"similarity\": f\"{similarity:.1f}%\"\n",
        "          })\n",
        "      return results\n"
      ],
      "metadata": {
        "id": "vH1pug9P3EOr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Interface\n",
        "\n",
        "app.py\n"
      ],
      "metadata": {
        "id": "kRFF5mPB6rjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# from db_management import SongDatabase\n",
        "\n",
        "# Data Base Instance\n",
        "db = SongDatabase()\n",
        "\n",
        "# Functions for Gradio\n",
        "def add_song(url):\n",
        "  try:\n",
        "    success = db.add_song(url)\n",
        "    if success:\n",
        "      stored_songs = [f\"{song['title']} - {song['author']}\" for song in db.song_data]\n",
        "      gr.Info(f\"✅ Song added successfully from '{url}'!\")\n",
        "      return \"\\n\".join(stored_songs),\"\" #clean \"\"\n",
        "    else:\n",
        "      raise gr.Error(f\"❌ Failed to add song from URL '{url}'. Please check the URL.\")\n",
        "  except Exception as e:\n",
        "    raise gr.Error(f\"❌ Failed to add song from URL '{url}'. Error during search: {e}\")\n",
        "\n",
        "def search_covers(query_url, top_k):\n",
        "  try:\n",
        "    results = db.search_covers(query_url, int(top_k))\n",
        "    if not results:\n",
        "      return \"<div style='color: red;'>No similar songs found.</div>\",\"\"\n",
        "\n",
        "    formatted_results = []\n",
        "    for i, res in enumerate(results):\n",
        "      # Agregar flechas o asteriscos para simular énfasis\n",
        "      similarity = float(res[\"similarity\"].strip('%'))\n",
        "      status = \"✅\" if similarity > 30 else \"❌\"\n",
        "      formatted_results.append(\n",
        "          f\"{status} {i+1}. {res['title']} - {res['author']} (Similarity: {res['similarity']})\"\n",
        "      )\n",
        "\n",
        "      song = get_details_from_youtube_url(query_url)\n",
        "      song_info = f\"{song['title']} - {song['author']}\"\n",
        "    return gr.Textbox(label=song_info, value=\"\\n\".join(formatted_results),),\"\"\n",
        "\n",
        "  except Exception as e:\n",
        "    raise gr.Error(f\"❌ Error during search: {e}\")\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as app:\n",
        "  gr.Markdown(\"# 🎵 **Cover Searching App**\")\n",
        "\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"### 🎼 **Add Songs to the Album**\")\n",
        "      song_url_input = gr.Textbox(label=\"YouTube URL\", placeholder=\"Enter the song's YouTube URL...\")\n",
        "      add_button = gr.Button(\"➕ Add Song\")\n",
        "      stored_songs_output = gr.Textbox(label=\"Stored Songs\", lines=10, interactive=False)\n",
        "      add_button.click(add_song, inputs=song_url_input, outputs=[stored_songs_output, song_url_input])\n",
        "\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"### 🔍 **Search for Similar Covers**\")\n",
        "      with gr.Row():\n",
        "        query_url_input = gr.Textbox(label=\"Search Cover URL\", placeholder=\"Enter a YouTube URL to search...\")\n",
        "        top_k_input = gr.Number(label=\"Top Results to Show\", value=1, precision=0,minimum=1)\n",
        "\n",
        "      search_button = gr.Button(\"🔍 Search\")\n",
        "      # search_output = gr.HTML(label=\"Results\")\n",
        "      search_output = gr.Textbox(label=\"Song Title and Artist\", lines=10, interactive=False)\n",
        "      search_button.click(search_covers, inputs=[query_url_input, top_k_input], outputs=[search_output, query_url_input])\n",
        "\n",
        "# App Execute\n",
        "app.launch()\n"
      ],
      "metadata": {
        "id": "BfJJT1Z4Nsdu",
        "outputId": "5ea937a6-b3df-4c20-bde9-dacc5fffee0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e440c449180faa6f9c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e440c449180faa6f9c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/Y0ORhLyJWuc\n"
      ],
      "metadata": {
        "id": "rjCWTZVVi3HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://youtu.be/TWX0SAh3T1I"
      ],
      "metadata": {
        "id": "_X1wnF09i0ZJ"
      }
    }
  ]
}