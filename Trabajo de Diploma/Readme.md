### Resumen

En la actualidad, las redes neuronales han revolucionado diversos sectores gracias a su
capacidad para comprender relaciones complejas en grandes conjuntos de datos. Sin
embargo, la opacidad en la toma de decisiones de estos modelos ha generado
desconfianza, impulsando la necesidad de la Inteligencia Artificial Explicable. La
explicabilidad, en este contexto, se presenta como un componente esencial para superar
los obst´aculos mencionados, pero su implementaci´on sigue siendo un desaf´ıo
significativo. La falta de m´etodos concretos para evaluar la calidad de las explicaciones
ha intensificado la necesidad de enfoques robustos y confiables capaces de proporcionar
claridad y veracidad en las interpretaciones. En respuesta a este desaf´ıo se propone el
dise˜no de un nuevo enfoque para interpretar las decisiones de modelos de clasificaci´on de
im´agenes como VGG19, basado en redes neuronales y utilizando entre la definici´on de
sus capas una funci´on de perturbaci´on. Se detalla la implementaci´on de la propuesta, as´ı
como de las funciones auxiliares. El enfoque propuesto se eval´ua en diferentes casos de
estudio a partir de realizar configuraciones espec´ıficas al conjunto Food101, explorando
de esa forma el m´etodo en diferentes escenarios se observ´o cierta consistencia en la
interpretaci´on de los mapas de relevancia. La comparaci´on entre los casos de estudio
revel´o cambios espec´ıficos a realizar para mejorar la calidad de las explicaciones
proporcionadas. Se obtuvieron resultados adecuados para una etapa experimental y se
reconoce la necesidad de futuras investigaciones para refinar los escenarios de prueba y
profundizar en la comprensi´on de las interacciones entre los par´ametros del modelo, la
funci´on de perturbaci´on y la interpretaci´on de la explicaci´on.

### Abstract

Nowadays, neural networks have revolutionized various sectors thanks to their ability to
understand complex relationships in large data sets. However, the opacity in the
decision-making of these models has generated distrust, driving the need for Explainable
Artificial Intelligence. Explainability, in this context, is presented as an essential
component to overcome the aforementioned obstacles, but its implementation remains a
significant challenge. The lack of concrete methods to assess the quality of explanations
has intensified the need for robust and reliable approaches capable of providing clarity
and veracity in interpretations. In response to this challenge, we propose the design of a
new approach to interpret the decisions of image classification models such as VGG19,
based on neural networks and using a perturbation function between the definition of its
layers. The implementation of the proposal is detailed, as well as the auxiliary functions.
The proposed approach is evaluated in different case studies based on making specific
configurations to the Food101 set, thus exploring the method in different scenarios some
consistency was observed in the interpretation of the relevance maps. The comparison
between the case studies revealed specific changes to be made to improve the quality of
the explanations provided. Adequate results were obtained for an experimental stage and
it is recognized the need for future research to refine the test scenarios and deepen the
understanding of the interactions between the model parameters, the perturbation
function and the interpretation of the explanation.
